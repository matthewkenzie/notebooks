{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4b254d",
   "metadata": {},
   "source": [
    "# DataFrame LookUp Test\n",
    "\n",
    "Go through the process of checking query speeds on a dataframe in which each row contains the entire article or in which each row is split by sentence. There are certainly quicker ways of doing some of the things in here. Notice for example that the dataframe is loaaded into memory and it is also copied in a couple of places (so that all of them are held in memory). For very large frames this should be batched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5577f",
   "metadata": {},
   "source": [
    "## 1. Get some random data from a source .txt files of BBC articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3bd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some mickey mouse data\n",
    "import subprocess\n",
    "dwnld = subprocess.run('wget http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip', shell=True, capture_output=True)\n",
    "unzip = subprocess.run('unzip bbc-fulltext.zip', shell=True, capture_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6e9e0",
   "metadata": {},
   "source": [
    "## 2. Load this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee5774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add it to my dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame( columns=['Filename','RawText','NormText'] )\n",
    "\n",
    "# words I will \"remove\" when normalising text\n",
    "normalised_removals = ['the','and','of','if','then','when','etc']\n",
    "\n",
    "## function to load a file and append it to the frame\n",
    "def load_file(df, fname):\n",
    "    \n",
    "    raw_string = ''\n",
    "    norm_string = ''\n",
    "    with open(fname) as infile:\n",
    "        # note the following does need to read the entire file into memory first\n",
    "        for line in infile: \n",
    "            if line in ['','\\n','\\r\\n']:\n",
    "                continue\n",
    "            else:\n",
    "                line = line.replace('\\n','. ')\n",
    "                raw_string += line\n",
    "                for remove in normalised_removals:\n",
    "                    line = line.replace(' '+remove+' ',' ')\n",
    "                norm_string += line\n",
    "                \n",
    "    df = df.append( { 'Filename': fname,\n",
    "                      'RawText': raw_string, \n",
    "                      'NormText': norm_string\n",
    "                    }, ignore_index=True )\n",
    "    return df\n",
    "\n",
    "## function which takes the raw datafrarme and return a copy with a \n",
    "## sub-index split by sentence\n",
    "def split_at_sentence(df):\n",
    "    sub_df = df.copy()\n",
    "    sub_df['RawText'] = sub_df['RawText'].str.split('.')\n",
    "    sub_df['NormText'] = sub_df['NormText'].str.split('.')\n",
    "    sub_df['SentenceID'] = sub_df['RawText'].apply(len).apply(range).apply(list)\n",
    "    sub_df = sub_df.explode(['SentenceID','RawText','NormText'])\n",
    "    sub_df['FileID'] = list(sub_df.index.values)\n",
    "    sub_df = sub_df.set_index(['FileID','SentenceID'])\n",
    "    return sub_df\n",
    "\n",
    "## loop files and append them to the df\n",
    "import glob\n",
    "for file in glob.glob('bbc/business/*.txt'):\n",
    "    df = load_file(df, file)\n",
    "\n",
    "## split the df into sentences\n",
    "sent_df = split_at_sentence(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4352020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now check look up speeds\n",
    "\n",
    "import time\n",
    "\n",
    "# function to time a process\n",
    "def howlong( func, *args ):\n",
    "    start = time.time()\n",
    "    ret = func(*args)\n",
    "    end = time.time()\n",
    "    took = end -  start\n",
    "    return took, ret\n",
    "\n",
    "# function to perform the search of a word in \"NormText\"\n",
    "# and return the resulting frame of matching results\n",
    "def search(df, word):\n",
    "    res = df['NormText'].str.contains(search_word, na=True)\n",
    "    return df[res]\n",
    "\n",
    "# function to process the return (i.e. return the \"RawText\" sentence\n",
    "# which is relevant)\n",
    "def process_return(df, word, is_split=True):\n",
    "    # we want to return the relevent sentences\n",
    "    # if the dataframe is already split by sentence this \n",
    "    # is trivial\n",
    "    if is_split:\n",
    "        return df['RawText']\n",
    "    # otherwise we now have to find the sentence it appeared in\n",
    "    # the quickest way I can think to do this is do what we have \n",
    "    # already done\n",
    "    else:\n",
    "        split_df = df.copy()\n",
    "        split_df['RawText'] = split_df['RawText'].str.split('.')\n",
    "        split_df['NormText'] = split_df['NormText'].str.split('.')\n",
    "        split_df = split_df.explode(['RawText','NormText'])\n",
    "        split_df = search(split_df, word)\n",
    "        return split_df['RawText']\n",
    "\n",
    "# function to amalgamate the search and process functions into one\n",
    "# for timing audit\n",
    "def search_and_process(df, word, is_split):\n",
    "    resdf = search(df, word)\n",
    "    return process_return(resdf, word, is_split)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "226f791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df had 510 entries\n",
      "Curated sent_df has 12725 entries\n",
      "\n",
      "Raw search time in original df =  2.1ms\n",
      "Raw search time in curated  df = 10.6ms\n",
      "Search and process time in original df = 14.5ms\n",
      "Search and process time in curated  df =  6.7ms\n"
     ]
    }
   ],
   "source": [
    "## Now actually perform the search\n",
    "search_word = 'market'\n",
    "\n",
    "t1, _ = howlong(search, df, search_word)\n",
    "t2, _ = howlong(search, sent_df, search_word)\n",
    "t3, resdf1 = howlong(search_and_process, df, search_word, False)\n",
    "t4, resdf2 = howlong(search_and_process, sent_df, search_word, True)\n",
    "\n",
    "print(f'Original df had {len(df)} entries')\n",
    "print(f'Curated sent_df has {len(sent_df)} entries')\n",
    "print('')\n",
    "print(f'Raw search time in original df = {t1*1000:4.1f}ms')\n",
    "print(f'Raw search time in curated  df = {t2*1000:4.1f}ms')\n",
    "print(f'Search and process time in original df = {t3*1000:4.1f}ms')\n",
    "print(f'Search and process time in curated  df = {t4*1000:4.1f}ms')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39370db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
